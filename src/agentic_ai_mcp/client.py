"""AgenticAIClient - Client for running agentic workflows with MCP tools."""

import asyncio
from typing import Any

from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.prebuilt import create_react_agent

from agentic_ai_mcp.config import Settings, get_default_model, get_settings
from agentic_ai_mcp.providers import get_provider
from agentic_ai_mcp.tools import ToolRegistry
from agentic_ai_mcp.workflows import PlanningWorkflow


class AgenticAIClient:
    """Client for running agentic workflows with MCP tools.

    Example:
        from agentic_ai_mcp import AgenticAIClient

        client = AgenticAIClient(
            mcp_url="http://127.0.0.1:8888/mcp",
            # api_key="sk-..."  # Optional if .env is set
        )

        # Simple task
        result = await client.run("Calculate 2+3")

        # Complex task with planning
        result = await client.run_with_planning("Calculate X, then do Y")
    """

    def __init__(
        self,
        name: str = "agentic-ai-client",
        mcp_url: str = "http://127.0.0.1:8888/mcp",
        provider: str = "anthropic",
        model: str | None = None,
        api_key: str | None = None,
        verbose: bool = False,
        settings: Settings | None = None,
    ) -> None:
        """Initialize AgenticAIClient.

        Args:
            name: Name for the client
            mcp_url: URL of the MCP server to connect to
            provider: LLM provider ('anthropic' or 'openai')
            model: LLM model name (defaults to settings)
            api_key: API key for the provider (optional, overrides .env)
            verbose: Enable verbose output
            settings: Optional Settings override
        """
        self.name = name
        self.mcp_url = mcp_url
        self.model = model or get_default_model()
        self.verbose = verbose

        # Settings - handle api_key override
        if settings is not None:
            self._settings = settings
        else:
            self._settings = get_settings()

        # Override API key if provided
        if api_key is not None:
            if provider == "anthropic":
                # Create new settings with overridden API key
                self._settings = Settings(
                    anthropic_api_key=api_key,
                    openai_api_key=self._settings.openai_api_key,
                    default_model=self._settings.default_model,
                    default_provider=self._settings.default_provider,
                    mcp_host=self._settings.mcp_host,
                    mcp_port=self._settings.mcp_port,
                    max_retries=self._settings.max_retries,
                    retry_base_delay=self._settings.retry_base_delay,
                    retry_max_delay=self._settings.retry_max_delay,
                )
            elif provider == "openai":
                self._settings = Settings(
                    anthropic_api_key=self._settings.anthropic_api_key,
                    openai_api_key=api_key,
                    default_model=self._settings.default_model,
                    default_provider=self._settings.default_provider,
                    mcp_host=self._settings.mcp_host,
                    mcp_port=self._settings.mcp_port,
                    max_retries=self._settings.max_retries,
                    retry_base_delay=self._settings.retry_base_delay,
                    retry_max_delay=self._settings.retry_max_delay,
                )

        # Components
        self._tool_registry = ToolRegistry(verbose=verbose)

        # Provider
        self._provider = get_provider(
            provider_type=provider,
            model=self.model,
            settings=self._settings,
        )

        # Agent state
        self._agent: Any = None
        self._planning_workflow: PlanningWorkflow | None = None

    @property
    def tools(self) -> list[str]:
        """List of loaded tool names from MCP server."""
        return [t.name for t in self._tool_registry.langchain_tools]

    def _get_llm(self) -> Any:
        """Get the LLM instance from the provider."""
        return self._provider.get_chat_model()

    async def _load_tools(self) -> None:
        """Load tools from MCP server as LangChain tools."""
        await self._tool_registry.load_from_mcp(self.mcp_url)

    async def run(self, prompt: str) -> str:
        """Run the agent with a prompt (simple ReAct workflow).

        Args:
            prompt: Task for the agent

        Returns:
            Agent's response
        """
        # Load tools if not loaded
        if not self._tool_registry.langchain_tools:
            await self._load_tools()

        # Create agent if not created
        if self._agent is None:
            self._agent = create_react_agent(
                self._get_llm(),
                self._tool_registry.langchain_tools,
            )

        if self.verbose:
            print(f"\n{'=' * 50}")
            print(f"PROMPT: {prompt}")
            print(f"{'=' * 50}\n")

        # Run
        result = await self._agent.ainvoke({"messages": [HumanMessage(content=prompt)]})

        # Process
        messages = result.get("messages", [])
        final_response = "No response"
        step = 0

        for msg in messages:
            if isinstance(msg, AIMessage):
                if hasattr(msg, "tool_calls") and msg.tool_calls:
                    for tc in msg.tool_calls:
                        step += 1
                        if self.verbose:
                            print(f"STEP {step}: {tc['name']}({tc['args']})")
                if msg.content and not (hasattr(msg, "tool_calls") and msg.tool_calls):
                    final_response = str(msg.content)
            elif isinstance(msg, ToolMessage) and self.verbose:
                print(f"  â†’ {msg.content}\n")

        if self.verbose:
            print(f"{'=' * 50}")
            print(f"RESULT: {final_response}")
            print(f"{'=' * 50}\n")

        return final_response

    async def run_with_planning(self, prompt: str) -> str:
        """Run the agent with planning for complex tasks.

        Uses LangGraph StateGraph to:
        1. Plan: Break down the task into steps
        2. Execute: Run each step with tools
        3. Synthesize: Combine results into final response

        Args:
            prompt: Complex task for the agent

        Returns:
            Agent's response
        """
        # Load tools if not loaded
        if not self._tool_registry.langchain_tools:
            await self._load_tools()

        # Create planning workflow if not created
        if self._planning_workflow is None:
            self._planning_workflow = PlanningWorkflow(
                llm=self._get_llm(),
                tools=self._tool_registry.langchain_tools,
                tool_registry=self._tool_registry,
                max_retries=self._settings.max_retries,
                verbose=self.verbose,
            )

        return await self._planning_workflow.run(prompt)

    def run_sync(self, prompt: str) -> str:
        """Synchronous version of run().

        Args:
            prompt: Task for the agent

        Returns:
            Agent's response
        """
        return asyncio.run(self.run(prompt))

    def run_with_planning_sync(self, prompt: str) -> str:
        """Synchronous version of run_with_planning().

        Args:
            prompt: Complex task for the agent

        Returns:
            Agent's response
        """
        return asyncio.run(self.run_with_planning(prompt))
